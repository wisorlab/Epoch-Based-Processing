MY NOTES TO SELF -*- mode: org -*-
July 16 2013:

Jonathan's code (copied into my directory) seems to run up to the point 
where I get an error about matrix dimensions agreeing in run_S_model.m line
32.  I think this is the same error there was when Jonathan first showed 
me this code. 

I made Physiovars average the delta power in ProcessLBatchMode.m so 
I changed make_frequency_plot_JW to use delta1,delta2, or lactate 
as the signal and use only 1 column of DataMatrix since that average has 
already been done before this function is called.  

To do next:  Right now ProcessLBatchMode is set to process all the text files
in the same directory as ProcessLBatchMode.m.  I don't think this a good 
way to go since you have to have all your .m files in the same directory 
as your data files.  

I have it set up now to read in the directory where the data files are stored. 
So set up different directories for each of the genetic strains, then 
run ProcessLBatchMode.m for each directory.  DONE
Or, improve ProcessLBatchMode.m so it knows about the differnet file names 
and computes all the correct Tau values and keeps them separated so you 
can compare strains easily.  


Thursday, July 17 2013: 
For today, work on my own code, not Jonathan's.  Implement the stuff I worked on two days 
ago to read in the excel files and batch process.  Also, set up different directories 
for each genetic strain.  Make sure I'm getting output that matches what I got before 
(on my linux machine).  


NOTE:  Jonathan uses the 5th and 95th percentiles for the lactate in make_frequency_plot.m
(see lines 120 and 121) (LA(i-shift) and UA(i-shift) ) where I currently 
use the 1st and 99th percentiles.  After I make sure this new version of the code 
is working, I should change this to 5th and 95th to match what he's doing.  

It would be great to put the datafile name on the figures that pop up.  That way if 
one looks funny, I can know which data file it is.  This shouldn't be too hard 
in ProcessLBatchMode, since I already have the file names in "files". Just read that into 
the functions that make the plots. DONE
 
Check to make sure the output is what I got before, using my desktop machine.  Both 
with the nelder-mead and without.  I put a .mat file on Dropbox. CHECK 

Tuesday, July 23 2013:
The Td values for the BA mice match up between the ProcessLBatchMode.m code and the 
code I ran on my desktop machine (up to round-off since the new version uses Nelder-Mead and is 
more accurate). 

Ti values for BA also match

check smoothing algorithm.  It doesn't seem like it's doing a good job.  
 
test different starting values for Nelder-Mead and tolerances to see if it changes the results 
or not.  

Wednesday, July 24 2013:
I spent a lot of time today trying to make emacs work well with matlab.  When I do 
"comment-region" it puts in hash marks not percent signs.  It looks like the file
to change is Desktop/emacs/lisp/progmodes/octave-mod.elc  The line that needs changing
has a defvar statement.  I don't know which number refers to the percent sign though. 
I may open up word and check that way.  Apparantly you can have Word tell you the 
Unicode for a character.  
Another option is to re-do my _emacs file to try matlab one more time.  Since it was only using 
octave-mod.elc and not octave-mod.el, I'm wondering if it only looks for .elc files, not .el 
files. 

SOLUTION:
You can byte-compile any EmacsLisp source file using the command ¡®byte-compile-file¡¯. For 
example with Emacs-Lisp you do:

  (byte-compile-file "foo.el")
EmacsLisp source files have extension `.el¡¯. Byte-compiled files have extension `.elc¡¯. If 
both source and byte-compiled files are present for the same library at the same place in 
your ¡®load-path¡¯, Emacs will load the byte-compiled file, not the source file.

This can be a source of confusion. If you change a source file, but don¡¯t remember to 
byte-compile it again, then Emacs might load the out-of-date byte-compiled file instead 
of the newer source file. Emacs warns you about this, but you might miss the warning message.

This is exactly what happened.  I re-compiled octave-mod.el and now it finally works. 
I have comments in matlab.  


Thursday, July 25 2013:
I ran the code without nelder-mead and it worked and was giving me correct results,
but much slower than w/o nelder-mead.  

I changed the tolerance in Nelder-Mead to be 1e-5 instead of 1e-3 and it made a significant 
difference in the output, so I will keep going until it doesn't change anymore.  
1e-9 seems to do a good job (4 digits of accuracy as compared to 1e-7) and almost 
no increase in computing time.  I think this is a good option. 

One idea:  put the code in to either export the Taui and Taud to an excel file, or 
go ahead and write a matlab script to call ProcessLBatchMode for two 
different directories and compute the students t-test.  

Tuesday, July 30, 2013
I've worked on the last idea from last time.  I have a function called 
compare_two_strains.m and it seems to work, but SmoothLactate.m is 
hanging up on one of the AKR files so I'm investigating.  Trying 
to use my own version of smoothlactate.m  

The problem seems to be in reading in the file, not in SmoothLactate.m.  There is 
a state variable missing on the first line, and I'm wondering if this is the 
problem.  

PROBLEM: If there is missing data (like the sleep state, as in the first line of
AKR1610_04_25_2013.txt) importdata.m (which is called in importDSILactateFftfile.m)
just stops at that line and gives no warning.  This could be a huge problem.  
Build in checks to make sure this doesn't happen ever.  This needs to be 
very robust.  It's the same issue I ran into before in linux with xlsread.m.

I found that this may have been happening for several files.  Importdata.m 
does not give any warnings if it finds a missing character, it just 
stops importing the file.  I'm now using textscan.m which is more robust, 
although trickier to use.  

My new version of ProcessLBatchMode.m seems to be working with my new 
importdata.m which uses textscan instead of importdata.  I have 
ProcessLBatchMode.m keep track of how many missing data 
points we have. I may want to update this to also tell me where
those missing data points are.  

This is a more robust method than importdata, which wouldn't 
even tell me if it encountered a missing value, it would just
quit right there.  Now we can decide what to do about those missing 
data points and write it right into the code.  I was finding that the 
first 3 data files in AKR had missing data.  Check all and make sure 
it's missing sleep state, not something else.  

Thursday, Aug. 1 2013
  One thing I noticed in file AKR1608_04_23_2.txt was that there were some negative values for lactate. 
I should check for negative values and just end the file before them if they are there.  
I added a couple of lines of code to importdatafile.m to cut them out. DONE

Now, something weird is happening: Several of the files have missing sleep state data, and 
missing_values is nonzero.  However, the model runs just fine for all of the files 
until file AKR1610_04_25_2.txt, when it complains that run_S_model.m found a sleep state
value that was not 0,1, or 2.  It seems like all the files with missing data should 
trigger this error since I set missing values to 5 in ProcessLBatchMode.m
The issue might be the fact that I'm using a moving window.  If the missing data 
occurs at the very beginning or the very end of the data file, my run_S_model
isn't doing its thing there anyway, so it won't complain, but if there is 
a missing value in the middle of the dataset (as was likely with the file that 
had 23 missing values), then run_S_model will be trying to do its thing 
and will complain.  

Perhaps I should write in some code to handle the case of missing data.  Perhaps 
I could check to see if there are several rows of W just before and just after. 
In that case I may be able to assume that the missing data are W.  
This doesn't seem promising.  For the file AKR1610_04_25_2.txt the missing data 
were not always in a contiguous block of one letter.  I may need some help filling in this
missing data.  
Decide (talking with Jonathan and Will) what to do about the missing data. It 
seems like Jonathan had it set up so if there wasn't a W, R, S, or P, set it to W. 
Maybe this is what I should do too.  

I ran into an issue with the smoothing algorithm for file DBA1621_04_24_2013.txt. 
It complained about subscripted assignment dimensions mismatch on this 
statement: PhsioVars(:,2)=LactateSmoothed(:,2);  I simplified SmootheLactate.m to 
only read in a vector and only spit out a vector.  
This seems to be working, but check it over by running ProcessLBatchMode on 
the DBA strain just to make sure.  

Monday, August 12, 2013
TO DO:  Make a figure with 4 panels, left column has best lactate fit to data 
for two data sets (6 or 8 hours).  Right panel uses SWA EEG2.  Choose two 
good fits (or relatively good fits).  
Don't use BALBC.  Look at Jonathan's figure and use the same strains. 
First check to be sure to use 24 hour files (maybe make new directories), 
then look at fit of SWA with EEG2.  Find some that look good.  

Why are BL files that end in 24 hours smaller than regular BL files?  
Open them up and look.  

Something seems wrong: looking at the model fit with delta data, a circle
(which represents the midpoint of a SWA episode longer than 5 minutes)
sometimes is plotted when the graph of the model is going up meaning
that the data there is Wake or REM.  Look at BL118640.txt around 8 hours. 
The issue seems to be that there are lots of little awakenings happening,
so what looks like a circle over a piece of graph going up is really 
a circle above a teeny tiny portion of the graph that is going down 
separated by very small portions that go up.  If you zoom in enough,
 you see that this is a SWA episode of about 6 minutes and the data 
point is situated right over the midpoint.  

I wrote find_all_SWS_episodes2.m.  It seemed to work OK on BL files. 
Check it with lactate on BL.  BL118540.txt looked decent with delta2. 


Tuesday, August 13 2013
I had a lot of trouble getting the SWA trigger to work and the model 
to look decent.  find_all_SWA_episodes2.m find all SWS episodes over 
5 minutes with at least 90% sleep.  Check this again.  There may be 
another bug.  

Jonathan requested that the figures with lactate as a signal have 
the first two hours cut off.  I think it would look funny to have 
the x-axis start at 2, so I changed run_S_model.m to plot against 
t-2 rather than t.  Change this back.  

9:11 PM using 80% with the sliding window isn't much better.  
It looks like there are columns of data points and we should 
be able to combine them much better.  Perhaps not using the moving 
window will work eventually. 

9:16 PM using 70% looks really noisy, but sort of OK for AKR1610_04_25_2.txt 
keep this in mind if I really need it.  

9:37 PM I'm trying the old way (find_all_SWS_episodes.m) using longer or 
shorter consecutive runs of SWS.  This didn't help much.  There are a couple 
that look OK, but super noisy.  Check to make sure I'm finding the SWS episodes 
correctly? 

11:38PM I tested find_all_SWS_episodes.m and it seems to be working perfectly, 
t_mdpt_SWS, data_at_SWS_midpoints and t_mdpt_indices seem to be correct.  


Friday, August 23, 2013
Notes from talk with Jonathan: 
- We'll make a figure like Franken Fig. 1 with lactate and delta power.

- I should assume BA and BL data files are final

- replicate slide 6 in Jonathan's strain study talk

- Franken Figure 2 is normalized, figure out how he normalizes.  I should
make a similar figure. 

- Since the lactate sensors are not rated for more than 60 hours 
if I'm using lactate as a signal, I need to cut off the simulation at
60 hours.  I don't have to do this for delta power. 

- Also, try cutting off the simulations even earlier if lactate is used: 
36,48,60 hours to see if this improves the fit. 

- Also try window lengths of 4,6,8 hours to see which works best. 


Thursday, Sept. 5, 2013
One idea to test whether the fit of the lactate model 
is only because of the thresholds or not:  
set up a really stupid model that is always either on the upper threshold 
lower threshold and instantly changes from one to the other if there is a 
change in sleep state.  i.e. if awake or REM, it is at the upper limit (moving
average), if asleep it is at the lower limit.  Then compute the error with 
this stupid method vs. the error with the exponential model.  Perhaps this 
could help us understand the relative contributions of the moving averages 
vs. the exponential model.  

I implemented this idea and it makes a plot of this new stupid model. 
The computed error is always lower with the exponential model, and 
usually a lot lower.  

I implemented the cut-off of 60 hours as well. (This is done in ProcessLBatchMode.m)   I 
haven't check error yet.  I haven't tried different windows lengths yet either. 

I haven't tried different window lengths or the new long datasets yet either. 


Thursday, Sept. 19, 2013
Cutting off the simulatons earlier if lactate is used isn't the problem.  For the most 
part the lactate simulations look quite good (for AKR,BA,BL,DBA).  The fit is nice.  It's the 
delta simulations that still look bad.  

I could make a plot of error in lactate model vs. the stupid model (where it goes immediately 
to the upper or lower limit if the state changes).  One idea would be to normalize the error
using the error in the real model, and show that the error using the stupid model is 30% 
higher or whatever it may be.  So compute the average error using my model (averaged over 
datasets for one strain) and the average error using stupid model (averaged over datasets 
for one strain) and set the error in my model to be 100% and show the other error relative 
to that.  

My model is having a really hard time with file BL-118540.txt Since the best fit means that 
the signal doesn't change much for a long time, the taui value is huge.  This completely 
throws off the comparison between strains.  The problem is that even though the animal 
is awake for a long stretch, the lactate signal is going down (hours 24-28).  My model 
really can't handle that very well, because it assumes that if the animal is awake lactate 
is going up. The DBA data has one file with the same problem: DBA1626_05_06_2013.txt 
has a huge Ti. Same problem with AKR1673_05_13_2013.txt BA doesn't have this problem. CHECK THIS

Modify Method to do Nelder-Mead 3 times with a random starting point each time.  This should 
avoid getting caught in local minimums.  
UPDATE: I changed the code to do lactate NM twice.  Keep in mind that NM starts with 3 guesses
anyway and triangulates from there.  This doesn't seem as bad as starting with only one guess
in terms of getting stuck in a local minimum. The second time I ran NM with random starting points 
and the error between the best values found in each AKR case
was usually small (10^-5), except for AKR1673_05_13_2013.txt which was problematic anyway.
All of them were great for the BA files. 
All of BL files are OK except BL-118540.txt
All of DBA files are OK except DBA1626_05_06_2013.txt

Thursday, Oct. 3, 2013
Keep working on ProcessLBatchMode.m to make a nice bar graph at the end.  It is not looking
good right now.  I want to add error bars.  

Also, write some code to make a nice figure like Jonathan has on the last slide of his talk, 
comparing our results to Franken 2001. 

Thursday, Oct. 10, 2013
Concatenation of data files:  Use a MATLAB script to produce a uniform dataset.  Will 
sees the data filtered when he looks at it, but the .txt files I get are not filtered. 
Will can filter the data and build new .txt files.  The original data is sampled at 500 Hz.
Several data files appear to be mis-scored.  (filtering has already happened).
To do for Will: with these data files, sort by power in delta range and give him the 
top 10% and bottom 10%.  (look for values that stand out).  This will be a guide for him 
to go back through and re-score some sections.  
For example AKR1608_04_23_2013b 12:35:00 PM epochs miscored as slow wave should be REM.

Thursday, Oct. 17, 2013
Files to sort: first two,3,4,5,6,7,8,9terrible,11 (all of them)

I added two columns 1 for average (across 1-2,2-3,3-4 Hz) for EEG1 and the same
for EEG2.  These two columns are labeled and are H and AW
Will can sort by column H or AW (right now they are sorted by EEG2, which is AW)

NEW IDEA:  I need to pull out only SWS episodes that are longer than 5 minutes first
 and then find the outliers.  What I'm doing now is for all the data, not just SWS 
episodes longer than 5 minutes.  I can do this in MATLAB, but how do I keep the time 
info, since that is what Will needs? 
I bet textdata{i,1} contains the time stamp.  That is what I need to get at with MATLAB. 
Spit out to an excel file the following columns: timestamp, avg. delta EEG1, avg. 
delta EEG2.  This should be sorted by delta EEG2. Spit out a separate .xls file for 
each file read in with the name of the original file in the name of the output file.  

Thursday, Nov. 14, 2013
I started a file called FindTroubleSpots.m to locate the SWS episodes that are longer
than 5 minutes and also much higher or much lower EEG power.  

I'm not sure this is worth it.  I'm making progress on FindTroubleSpots.m, but it will 
give about the same info as the files I already made last time. 
I guess it would be good to have an automated way of doing this in case I need to do
it again. 

FindTroubleSpots.m is almost working (it may be now). It looks like it is actually
working.  Keep going on that a little 
bit to make nice excel files containing only SWS episodes longer than 5 minutes
sorted by power in EEG1. 

Monday, January 13, 2014
I changed SmoothLacate.m to just smooth.m so I can modify it and use 
it for smoothing EEG signals too.  It now has more arguments so I can 
use the same code, but tune it for EEG.  I checked smooth.m vs 
SmoothLactate.m and SmootheLactate.m and the results are identical. 

Playing with the parameters in smooth.m doesn't seem to make any 
difference.  The plots of the raw data vs. smoothed data seem 
to change a bit, but the SWS episodes of >5 min look exactly 
the same.  

Should the smoothing algorithm use the past 10 smoothed points 
instead of unsmoothed points to check if a point too extreme? 
I think this is keeping some smoothing from happening.  

Maybe filter by making sure the absolute variation between two 
successive data points isn't too large?  some files looked smooth
for most of the file except one little section with large deviations. 

Wednesday, January 15, 2014
I changed smooth.m to use the past 10 smoothed points instead 
of unsmoothed points, but if the signal is relatively constant 
for a few steps and then jumps up suddenly the smoothed signal 
will remain constant and stay constant for the rest of the experiment. 

Need to think of a better way to smooth successive extreme values.  
Increasing window size to 50 didn't seem to help. 
Changing window size to 5 helped smooth out signal a bit but 
did not change the plot of 5 min SWA episodes
Changing 10 SDs to 5 helped smooth things out even more, but 
didn't change the plot of 5 min SWA episodes. It may be 

It seems like the problem is one of state scoring, not 
filtering of the EEG signal.  Some region has been scored 
as SWA when it really wasn't.  

For file AKR1610_04_25-26_2013.txt it seems like the most 
extreme value of delta1 power (around 56 hours) occurs because there 
were two sucessive extreme values in the EEG signal.  The current 
smoothing algorithm can't handle this kind of event. 

Maybe do something like compute a moving average over 3 points or 
something.  If that moving average is greater than 10 SDs away from 
the average of the previous 10 points (30) then set all 3 of those 
points to the average of previous 10 (or 30).  

Transients? Can I just cut them off?  


Why does the smoothing algorithm seem to work so well for lactate
 but not EEG?  For lactate the artifacts are just one data point.  
If they are more than one data point they don't get filtered.  In the 
EEG signal the artifacts are frequently more than one data point.  
I need a good way to take care of them.  

I'm not convinced the smooth2.m function is working properly.  

Thursday, January 16, 2014
Jonathan said to leave the lactate filtering as it is.  It's not 
perfect, but it does a good job. 

I found a really nice library of smoothing functions on the web and 
talked to Jonathan about using them. 
One option is a Gaussian smoothing (3 passes of a rectangular 
sliding-average smoothing) and one option is a median filter. 
The documentation says that the median filter is good at removing spikes.

Applying the Gaussian smoothing with a 5-point window does a great 
job of making the data look better, but the plot of SWA episodes looks 
about the same.  

Trying with the median filter.  It didn't seem to make any difference to the 
plot of 5-min SWA episodes.  The raw data look much smoother, but the SWA episodes
don't change much.  I like the median filter on the EEG data.  It looks like 
it's doing exactly what you would want a filter to do.  Why are 5-min 
SWA episodes still looking so noisy?  

Try the median filter on the lactate data too... Our current filter really 
is missing quite a few of the artifacts. The median filter with width 
of 1 or 2 looks perfect.  I think we should use this on lactate rather than 
what we had before.   

Investigating the outlier in plot for AKR1610_04_25_26_2013.txt:  Even with the filtering
there is a bump in the data where it goes up to about 3000 for just a little bit. 
It must be that there was a SWA episode longer than 5 minutes happening right during 
this bump.  That's why that point is still there.  It may be an artifact, but it won't go 
away with more filtering.  

AKR still has a lot of really bad files even with the new filtering. (using delta1) 
AKR1672_05_09_2013.txt looks like the first 4 hours or so should be cut off.  There is 
a huge transient that is ruining the curve fit.  Many of the files contain data 
that look pretty random.  No model will give a good fit except a random algorithm.

BA files in BA/BA_long look pretty good using delta1 or delta2 except for BA-120440.txt
BL looks pretty decent using delta1 or delta2. (files in BL/only_24hr_files)
 
DBA using delta 1 is a mixed bag.  Some files look quite good and some are terrible. 
DBA1622_04_29_2013.txt looks very bad. DBA1626_05_06_2013.txt is bad too. 
with delta2: DBA1683_05_20_2013.txt is bad too.   DBA1681_05_16_2013.txt is bad.
DBA1671_05_09_2013.txt is bad too.
There is a fundamental difference between the good EEG data files and the bad ones.  
In the good ones you can see definite trends on the order of 1 hour and 
significant rises and falls. In a bad file the signal looks more like just white noise.  


some files seem to have a significant transient at the beginning.  Can I cut that off? 

Tuesday, January 21, 2014
Trying to reproduce the panels of Figure 1 from Franken.  First choose the best 
delta output.  The histogram plots look worse if I filter using the median filter 
and 10 points.  Keep trying median filter with fewer points.  Using 2 looks OK.  
Make sure all the other outputs including lactate look OK with this.  Lactate usese 1  

Use BL-118540-24Hrs.txt as the file to make Franken Figure 1.  Copy this file into 
his own directory and write a script to make all panels of Figure 1.  

Jonathan and I copied over some new files for AKR.  They are in 
data_files/AKR/long_files/concat

However, I can't read them since importdatafile.m breaks for these new 
files, even when there is only 1 as in concat2.  
importdatafile.m works just fine for the data_files/AKR/long_files/new_files

Play with textscan (even from the command line) and try to figure out why it's 
breaking.  

shadedErrorBar is the matlab file I want from matlab central. 

Thursday, January 23, 2014

I found a workaround for the importdatafile.m issue.  I opened all the 
.txt files in Excel and resaved them as tab-delimited text rather than Unicode 
text.  This makes my code work.  I tried for quite a while to find a way to 
program in a workaround so my code is more robust, but I couldn't make it work,
so this will have to do.  

BAD LIST:
Using the files in AKR/long_files/concat 3 of them are OK, but 
AKR1608_04_23_2013_and_AKR1608_04_24_2013.txt is bad deltas
AKR1611_04_29_2013 and AKR1611_04_30_2013.txt is bad deltas (lactate OK)
DBA1621_04_23_2.txt is bad if delta2 is used
DBA1626_05_06_2013.txt is bad (40hrs) 
DBA1681_05_16_2013.txt
DBA1683_05_20_2013.txt is bad delta1 (only 25 hours)
BA-120440.txt (35 hrs scattered) delta1
(BLs look good in long_files)
Several of the lactate files could benefit from cutting off the first 

Jonathan said to try using overlapping windows for SWA episodes.  It turns out 
that my code was already doing this.  This is what find_all_SWA_episodes2.m does. 
find_all_SWA_episodes.m does not count overlapping SWA episodes.  If there is 
SWA for 8 minutes straight it counts that as one episode (it sounds like this is 
what Franken is doing).  

Using non-overlapping windows to find SWA episodes (like I think Franken did), 
makes for much worse data for the AKR concatenated files.  In this case, 
every file looks bad.  

Keep using sliding windows.  

I may need to fake it for panel a.  My way isn't looking very good because of 
really short REM episodes.  This works and looks good.  

Tuesday, January 28 2014
Now put all the figures that the Figure1_script.m makes as subfigures
into one big figure  (this is almost done.  I used line.m to make the lines in panels
a and c rather than squares since using squares made it look like sleep states 
overlapped)

Next: re-make the same panels using lactate as the signal not delta
make the last panel bigger.  play with something like subplot(2,3,[5:6])

Thursday, January 30, 2014
Keep working on the Figure1_script.m for the lactate case.  For the current data 
file, BL118540 the lactate signal isn't too good so one Tau value is huge.  I think 
this is close to working.  choose another data file that looks good for lactate and 
for delta and use it instead.  

Thursday,Feb. 6, 2014
The BL11850 file doesn't even look all that good for the delta fit. Find a better file.
Candidates: 
BL-119240 delta1 has good histogram (try lactate) not quite 48 hours (it is a 48hr file, just no
SWS episodes of >5min in last 3 hours. 
BL-119240 isn't great, but maybe the lactate figure and delta figure don't have to use the 
same dataset.  
BL-119240 looks much better in the Figure1 with all the panels.  Use that one instead. 
lactate doesn't work yet for this script. Also verify that the tau values we get from 
brute force match those from nelder-mead. 

Thursday, Feb. 20, 2014
Looking for a good candidate for the lactate figure.  BL-119240 doesn't look great,
but I don't know if the lactate figure should be the same dataset as the EEG figure. 
Or, just do a zoomed-in version of the lactate and model overlay? 
BL-118140 looks much better for a lactate model fit.  Check its EEG. Maybe I could 
use that file for both.
BL-118140 looks good for both EEG2 and lactate.  Use this for both figures. 
Modify Figure1_script to work with lactate.  Better yet make a new function 
since this figure will be pretty different.  Cut all the lactate stuff out of 
Figure1_script.m

Make a new function that is Figure2_script.m that makes a 4 panel figure:
A: histogram of all data (SWS,REM,WAKE) showing LA and UA initially. 
B: lacate data and the moving upper and lower asymptotes
C: contour plot of error like previous figure but with several NM guesses on it. 
D: Best fit of model to data with colored data points like in 

Making progress on this, but panel A has same figure as panel D. FIXED

Looking pretty good, but I'd like to add insets of the histogram to the 
panel that shows the changes in UA and LA. 

Sometimes it's tricky to get a panel to look right if I add something 
to it like a legend or an inset. 
Try making those plots directly in the subplot rather than making them 
and copying them in.  All but panel D seem to be created in Figure2_script
rather than in another function. So make a figure and just make those 
3 panels using the subplot command.  The only trick will be to put the 
figure for panel D into its place.  

* Thursday, March 6 2014



+this should be strikethrough+
TODO: 
1) add points of simplex method guesses to the contour plot in the figures? ADDED TO FIGURE 2
2) +rotate histogram in lactate plot so bars go horizontally (same for inset histograms)+
3) +Put all the data files on the D drive.+  
4) +clean up my directory with code so that it contains only those functions I actually use+
5) +Copy (and improve?) Jonathan's lacate smoothing code so my code uses it.+ 
6) rename my main function from ProcessLBatchMode.m to something more useful. 
7) +Remove the trace of the model from top right panel in Figure 2+
8) Keep improving medianfiltervectorized to remove for loop and share with lab.
9) +Add legend to bottom right panel on Figure2+
10) +Think about doing all the coding for the bottom right panel in the Figure2_script.m.+
It is getting to be kind of a pain to get the legend right and I have to modify run_S_model.m 
which I don't really want to do. 

* Thursday, March 13 2014
I moved all of my data files to the D drive under the mrempe folder.  
Now every time I call one of my functions in mrempe/EpochBasedProcessing/Timed/Internal
I will need to set the path as a variable (like this: >> path='D:/mrempe/') 
then call the function like this: >> [fh,error]=Figure2_script(path);

My vectorized version of median smoothing gives the exact same output as my nonvectorized
and it is much faster (a factor of 100).  It is called medianfiltervectorized.m

I'm working on making the code faster.  make_frequency_plot.m seems to be bottleneck. 
Matlab tells me to avoid using find.m and just just logical indexing. Keep working on this. 
Work on making the loops in run_S_model faster.  Use this link:
http://stackoverflow.com/questions/10194122/vectorization-for-and-if

* Thursday, April 3, 2014
 Data files are now in D:\mrempe\strain_study_data\BL\fig1_file  
How to make my code see them: 
[fh,error]=Figure2_script('D:\mrempe\strain_study_data\BL\fig1_file\') worked.
Now make the directory a string variable so I don't have to type it each time. 
data_directory='D:\mrempe\strain_study_data\BL\fig1_file\'  this works

Put together a rough draft of the paper, along with figure placeholders and an 
outline and send it to Jonathan with instructions on what I want him to fill in
(methods, part of discussion, etc.)  

* Thursday, April 10, 2014

Plan for today:
Send Jonathan a rough draft letting him know where I'd like him to write POSTPONED
Read background papers and fill in outline of our paper
Determine which traces still look bad and where to cut off all traces
Back up my stuff to GitHub
Start code for shaded figure

Send Jonathan a reasonable draft when I get it to that point.  I can 
leave whole sections for him to write.  Turn each bullet point in 
discussion into a short paragraph.  

Jonathan said for the figure that looks like Franken's figure 2, I can 
cut off the traces that don't have sleep deprivation.  He justified 
this by saying that the lactate dynamics are on such a different 
time scale...?

BIG QUESTION: Do we want to make Franken's figure 2 using EEG like he 
does or just make it using lactate (or both)?  I'm going to try to write 
the code so it's easy to make both (call the same function just with a different
signal)

I've decided to modify ProcessLBatchMode.m to return more than it currently 
does.  It will now return a cell array containing either delta power data
or lactate for all animals in that strain and another cell array containing the
best fit S variables for each animal in that strain.  This way I can use that 
data in other functions.  I decided not to do those calculations inside of 
ProcessLBatchMode.m because I don't want to change the fundamental purpose 
of that script: to find the best fit of the model for each animal in the 
group and compute the tau_i and tau_d values for each animal.  


I started mean_time_course.m, but it just has comments in it.  
ProcessLBatchMode.m is updated and working.  Keep working on compute_mean_time_course


* Thursday, April 17, 2014
Talked with Jonathan: Only the BA and BL strains had sleep deprivation 
for us.  Sleep deprivation was 6 hours at the same time as Franken.

Baseline goes from 0 until 24 hours, so the last 4 hours of baseline 
would be from 20 hours until 24 hours.  
Try different approaches for this:  6 hours ending at 24 hours, shift 
2 hours one way or another.  

Our experiments followed the light/dark schedule of Franken.  

I worked on compute_mean_time_course.m today and it may be ready to go.  Keep 
checking it over and try running the script to make shaded figure.  

* Thursday, April 24, 2014

Goals for today:
1) GitHub
2) make a draft of figure(s) like Franken Figure 2
3) Start draft of abstract for MAA?


How to use GitHub:
I've set up a branch  under the wisorlab github in Epoch-Based-Processing directory
called mrempe.  In this branch I have the directory Timed_Intervlas/internal. 
To use GitHub on a typical coding day: (use the posh command line. it has a blue icon that looks 
like a play button)
1) make a new directory anywhere in mrempe 
and call it something like todayscode or something.  This can be deleted each day (if I push commits back
to GitHub.  
2) git clone https://github.com/wisorlab/Epoch-Based-Processing/ (I modified the .gitconfig file so 
I should be able to just type git clone rempe_internal    and it should work)
3) git branch (this lists the branches, you may not see mrempe)
4) git checkout mrempe
5) modify the code in the directories just created
6) git status (tells me which files have changed)
7) git commit -am 'status message' 
8) git push origin mrempe   (this pushes it to the server)


make_shaded_figure.m is running, but the output does not look good. 
Looks like maybe I'm scaling S and/or data incorrectly as S does not 
match data well at all (using delta)  Checking with BA.  Make plots 
of scaled S and scaled delta for each animal to make sure they 
look good since most of the animals have a decent model fit when 
you just run PROCESSLBATCHMODE.m for BA. 

It seems like I should be using the circles, not all of the delta1 data,
since the circles are what I fitted to the model to.  I will now modify 
compute_mean_time_course.m to use the circles not the raw data.  
I think I implemented this, but the plot still looks quite bad.  Surprising since
the individual plots look good for the most part.  
Keep working on this. 


* Thursday, May 1, 2014
Jon wrote a script for me to set up git each time I start a new day. 
From the powershell (play button symbol) type C:\Users\wisorlab\profiles\wisorlab_profile.ps1 
from a new directory that I create.  This will set up all the git stuff.  

New git procedure:
1) Double click on the Git Shell icon on the desktop
1a) Make a new directory in mrempe and cd to this directory.
2) type C:\Users\wisorlab\profiles\wisorlab_profile.ps1 
3) modify the code in the directories just created (cd down to Epoch-based/Timed/internal)
4) git status (tells me which files have changed)
5) git commit -am 'status message'
6) git push origin mrempe (this pushes it to the server)


My shaded figure seems to be working now.  I was normalizing the S output to something 
different than what I was using to normalize the data.  Fixing this bug makes the plots
look OK, the fits are still not great, but just OK.  Play with altering them in a way to 
make them better (lengths of moving averages, etc.) 

I'm still not convinced that the S curves are being normalized correctly. 
Check one file S curve and data normalized to make sure they match up.  Choose a
file that the model fits well.  

One idea: make another directory where I remove the bad files and make the shaded 
figure.  Are the bad datasets making the shaded figure bad, or is it something else?
This did not improve the fit at all.  Look at individual traces after normalizing S 
and the data.  

individual data sets with traces of S look good when they are both normalized. 
Check the averaging or something.  I don't know why individual fits look so good, but 
the gray areas look so bad.  

Tried speeding up run_S_model.m without success, so I changed it back. 

* Thursday, May 15, 2014
I'm working on making the shaded figure like Franken figure 2.  My plots for 
delta don't look great, and I tried lactate too.  The lactate figures look OK,
but also not great.  It seems like with lactate when I normalize and take only 
average across 45 minute intervals that actually makes the fit worse than just the 
raw lactate data and the model fit.  For lactate perhaps I shouldn't average like 
that and I shouldn't just do SWA episodes.  Just do lactate all the time, like I do in 
the figures with the circles and best fit of the model. 

Change compute_mean_time_course.m for lactate to use all the data, not just SWA episodes 
longer than 45 minutes or whateever. 

Working on this, but now my plot of normalized model fit and normalized 
lactate data don't line up even though they look great in colored circles.  
I'm comparing model fit and data for lactate for file BA-120440.txt
Aah: The model fit for lactate starts after 2 hours since we are using a moving 
window.  Take this into account.  I fixed this so the tS variable coming out 
of compute_mean_time_course.m is correct for S.  

NOTE: For lactate I don't average process S over 15 minute intervals and 
I don't average the lactate data over consecutive 45 minute intervals either. 

make sure it still runs with delta too.  I made a couple of small changes.
Good news: now the data and model fit really well for lactate, but the problem is 
that it's too much data.  Plot only every 10 point or something.  Code this up in a 
general way so I can easily change it. (Or average over every nth point)


* Thursday, May 22, 2014
I fixed several bugs in compute_mean_time_course.m that were keeping me 
from using shaded_figure_script.m.  Most of them had to do with setting up the 
correct versions of tdata,tS, etc. to make the plots.  

The plots seem to work for both lactate and delta, but for lactate it's way too 
much data so I'm changing the shaded_figure_script.m to plot lactate data every 
45 minutes instead of every epoch.  

I fixed these issues and made decent verions of the figure with delta1 and with lactate.

NOTES from meeting with Jonathan:
What am I using to normalize the lactate signal? I'M USING THE AVERAGE VALUE OF THE LACTATE 
SIGNAL FROM T=20 TO T=24.  This normalization seems to be working. The lactate figure has 
data around 100 for 20<t<24 or so.   

The data in the delta figure should go to 100 during baseline (last 4 hours of light phase) since that is what 
I'm using to normalize it. Check this. LACTATE LOOKS OK FOR THIS.  WHAT COULD BE HAPPENING 
FOR DELTA? One issue: I'm currently normalizing to the mean delta power 
during the last 4 hours of the baseline light period.  *I should be normalizing to 
the mean delta power in SWS over the last 4 hours of baseline light. (not just long 
SWS episodes, all of SWS). So, first find all the epochs labeled as SWS during the 
last 4 hours of baseline.  Then compute the mean delta power over these epochs. Normalize
to this.* I made this change and it helped quite a bit.  The values of the data (dots)
are much closer to 100 between 20 and 24 now and the range of the data has been reduced 
quite a bit.  AKR still looks a little funny, but the others are quite a bit better. 
Perhaps the exclusion idea or leaving out data points with fewer than 4 animals will 
clean that one up.    

For lactate simulation cut off the start of the simulation until 6pm of the first day.  
Check the Excel files to make sure they all start at the same time. The AKR files do 
not all start at the same time.  12:28, 16:10,14:36,15:47,16:23.  Check the other strains too.
Can't do this until we sort out the start times.  

For the bad files, come up with an EXCLUSION Criteria:  One option would 
be to use the 7 animals with widest dynamic range in delta power.  To compute 
the dynamic range: 1) compute the 10th percentile for SWA. 2) Compute the 90th percentile 
for SWA. 3) Subtract the 10th percentile from the 90th percentile and that is the dynamic 
range.   *Could we use the frequency histogram to throw out files?*

Leave out data points from this figure that do not have at least 4 animals contributing to the 
mean.   DONE!  May 26

* Monday, May 26, 2014

Excluding 45-minute intervals where fewer than 4 animals contributed a SWA episode of more than 
5 minutes made the fits of model to data look a bit better, although AKR and DBA are missing 
quite a bit of data.  Re-try it excluding intervals where fewer than 3 animals contributed. 
This helped a bit, but not much. 

BA and BL look pretty good in the shaded plot, AKR and DBA are not as nice.  

I made an excel file of the starting times, ending times, and duration of all the data files. 
There is quite a bit of variation, particularly with AKR and DBA.  This may explain why some of them
look so bad.  Talk to Jonathan and Will about this.  

For the exclusion stuff, keep working on PROCESSLBATchmode.m.  I saved a copy of the previous
version to Github.  The new version has commented out code.  Two for loops instead of one. 
Leave out files that don't meet some criteria. 

* Tuesday, May 27, 2014
Things to think about today:
- Look at Jonathan's and Jon's code to do summary statistics of the data in excel files. 
I'll modify this to work with lactate rather than delta power. 
- Jonathan has a script that imports data from an excel spreadsheet, much like my 
ProcessLBatchmode.m.  Check it out and see if we can combine and condense.

- Work on setting a start time for the data files that I read in so the cell array 
contains data that all starts at the same time (same ending time too?). Also, 
beware of daylight savings time.  Perhaps code this in: correct in winter, modify in 
summer? 

Problem: ProcessLBatchmode.m now just processes one file 7 times instead of processing 
the 7 with the largest dynamic range.  Broken for strains with fewer than 7 files and those
with more than 7 files.  files(FileCounter).name seems to work, since the figure titles 
are correct, it's just that the same data is getting loaded each time. 
FIXED: Franken_like_model was using PhysioVars in it's call instead of signal_data 
cell array.  I had to modify several .m files so Franken_like_model can read in 
just the sleep state and signal data (lactate or delta) and nothing else. 
It seems to be working. This cleaned up several .m files.

I played around with SigmaPlot a little and it looks like it may be a good way to go to make the 
bar graphs comparing tau values for lactate and delta and process S.  It can output eps files too. 
The format seems to be the following: in one column have the averages for each strain. In the next column
put the standard errors (or standard deviation).  Or just write a MATLaB function.  Matlab has the 
benefit that if I get new data I can just re-run it.  Sigmaplot would require me to go through the gui each 
time.  But it does look nice.  

Work on matlab script to make bar graphs for taui and taud. A DRAFT is completed (make_bar_graph_figure.m)
TEST THIS

Continue working on using sscanf or substring.m to get at the time stamp data so we can leave off
data if we need to in order to make all experiments start at the same time. 

* Wednesday, May 28, 2014
I'm implementing code today to read in the time stamps in the input data and only use data starting at 
a specified starting point.  
File AKR1608_04_23_2013_and_AKR1608_04_24_2013.txt has a section where all EEG data are 0 and lactate data 
is constant (lines 7224-7248)  Ask Will and Jonathan about this.  Do I just cut this section out? 
It has a long wake bout just after this where lactate is decreasing for the entire wake bout. Ask about 
this too. 
AKR1611_04_29_2013_and_AKR1611_04_30_2013.txt ends with a row of zeros. I should delete this row too.

Maybe think of a better way for my model to handle missing data. Should I skip that part and just start 
again when the data starts again?   For the delta model it probably doesn't matter since there are 
no SWA episodes in those regions so I won't have a data point to match to (assuming X gets turned into wake)

But it may matter for the lactate model.  Perhaps I could have S stay the same if sleep state is listed 
as X?  Or just leave that section out of the model and have process S start over again after the break
at the next value of lactate?  

+IMPORTANT: After we decide where all datasets should begin, decide when the baseline dark period is and CHANGE THE+ 
+CORRESPONDING CODE IN compute_mean_time_course.m+  

NOTES FROM TALK WITH JONATHAN AND WILL:
1) +Cut off all time before 20:00:00+ and cut off simulations at 43 or 44 hours? Ask Jonathan. 
2) +Lights off at 17:00, Lights on at 5:00, baseline is 13:00-17:00 which is 17 hours-21 hours into recording+ DONE in compute_mean_time_course.m
3) For now use only BA, BL, and DBA.  Will has 2 more genetic strains for me.
4) It would be good to make my code more general to include an arbitrary number of genetic strains
5) +Eliminate file BA-120440.txt.  It does not start at the right time.  +
6) +For file AKR1608... just cut out all the 0 entries and the row above and copy and+ 
+paste line 7222 (except the time stamp).+


* Thursday, May 29, 2014
+Working on code to compute time elapsed given year, month, day, hour, minute, and second.+ 

 
I was able to load matlab mode in emacs. just type M-x matlab-mode and it kind of works.
It only worked for about 30 seconds.  Now it doesn't work anymore. 

* Friday, May 30, 2014
Yesterday afternoon I made a bar plot figure comparing tau values for all strains and the BL 
taui value looked too high.  Check into this.  

Also, Will told me that more datasets are available on glitch (or glitchy?) More AKR, and the other strains. 
Look for them.  They aren't ready yet.  They haven't been scored.  

Notes from morning meeting: Jonathan wants me to just focus on the BA and BL strains right now and 
publish introducing the simplex method to the sleep research community.  Compare the time it takes to find the 
optimal tau values using brute force and simplex method since Paul Franken told Jonathan that they 
would run simulations overnight.  The idea is that once we validate the simplex method for these 
homeostatic models, (by comparing it to the brute force method), we can use it for every 
other publication where we use the homeostatic model.  

I'm working on a robust procedure to make sure we are truly finding the minimum. Perhaps what 
I have now is sufficient, but I may need to include something for the case when the two 
runs of NM give you a different minimum. 

Change my code to use matlab's fminsearch.m and make sure it gives the same minima.  
It looks like fminsearch.m is working.  Compare to current implementation in terms 
of speed and accuracy.  They match well for all of the BL files. 
fminsearch:from 2.99  to 4.23 seconds for BL
my code from Mathews: 3.19 to 5.23 seconds for BL

Speed up the first loop in ProcessSBatchMode.m where the files are read in and checked. It 
seems like it shouldn't be so slow.  Maybe run profiler just on that loop?
Nothing seemed promising here.  I could try re-writing it to use multiple other 
matlab functions and it may be a bit faster, but it may not be as robust.  

I also looked at speeding up the update of S using ode45.  You can't tell 
ode45 where to evaluate the function.  You can, but it will also evaluate it in-between 
these points.  This won't work for me because it doesn't make sense to update S in-between 
places where you know the sleep state.  It doesn' tmake sense to interpolate sleep state. 

I tried to think of other ways to vectorize the update of S, but nothing seemed promising. 

* Monday, June 2, 2014
EEG1 signal on BL-118040.txt is garbage.  Use EEG2 for this file. Maybe use EEG2 
in general.  Compare the two for each animal.  They all look very similar for 
EEG1 and EEG2, but use EEG2 anyway.  Making the model fit plots short and wide
helps show how well the model fits the data.  For the BL strain using delta2 the
fit is quite good for every file.  
Look at BA EEG2.  make the plots short and wide.  These look decent too.  

I got some new data files from Will for the BA strain.  They mostly look good.  

fminsearch is not working with lactate as the signal.  S is not being set to the correct length.
So the errorout calculation isn't working.  complains about dimension agreement.  It's working 
again.  I had preallocated space for S.  That was the problem.  

* Tuesday, June 3, 2014
 Will sent me an Excel file listing the strain data files and categorizing them. 
I agree with his assessment for the AKR strain.  Keep going through all of the 
strains.  

Perhaps change the exclusion criteria: instead of keeping best 7 files, just throw 
out a file if it's dynamic range is more than 2 SDs away from the average or something. 
If we have more than 7 good files we should use them all.  

Notes to self on talk with Jonathan: 
1) We will work on two papers this summer.  The first paper, sent to Neuroscience Letters 
or J Neuroscience Methods will use only one genetic strain and make only two real points:
a) lactate as homeostat
b) Nelder-Mead (simplex) vs. brute force in optimizing values for tau's

2) The second paper will have the transition figure and all the genetic strains.  More
similar to Franken paper, except with all the genetic strains and lactate

3) For the first paper, make a table like Franken table 1 except mine has only two rows: 
B6-NM and B6-iterative method.  columns are mean and SEM for 
Taui, Taud,UA,LA,S0,running time (other columns from Franken? One table for lactate 
and one for delta? I probably don't need UA or LA or S0 in here since they 
are chosen the same way for brute force or NM.  

4) For figure 1, modify my delta Franken fig 1 to not look quite so much like Franken's. 
perhaps split into separate figures?  Add a panel showing NM iterations and a panel 
showing best fit with NM optimum value. 

5) For figure 2, keep my top two rows for lactate figure, but remove NM guesses from contour 
plot and add a row of panels that shows NM guesses with triangles and the best fit using 
NM.    

6) Add triangles to NM iterations plots.  

7) could label lines on contour plot to show how much larger the error measure was 
at the line compared to the minimum error (i.e. at the smallest contour the error 
was 1.25 times the minimum error, 2.5 times the minimum error, all the way up to 
80 times the minimum error in Franken Figure 1 panel d). 

Make two figures (one for delta, one for lactate), each with 6 panels: 
freq plot sideways            dashed line plot like lactate figure top right panel
color contour brute force     best fit of model using brute force approach
NM triangles                  best fit of model using NM

Best option may be to modify Figure2_script.m to work for both lactate and delta and 
include the extra panels.  

I may need to have an option in PROCESSLBATCHMODE.m that lets you choose 
between NM or brute force.  This way I can have one script that makes all 
the panels of the figure (with and without NM).  Or maybe not.  Figure2_script.m
just does it in the script.  I can probably just do this.  

I couldn't get fminsearch.m to plot the successive guesses so I'm using 
nelder_mead_for_lactate.m and nelder_mead_for_delta.m for the NM 
panel of the plot.  

Compare 10 iterations of NM with random starting guesses to 1 NM run with 
my initial triangle and compare both to brute force.  

Send Jonathan a draft today before I leave!

* Friday, June 13, 2014
Change Franken_like_model.m and Franken_like_model_with_nelder_mead.m to 
return UA and LA normalized as a percentage of mean SWS delta power in last 
4 hours of the baseline light period.  

NOtes from talk with Jonathan: in the discussion of this paper (and in the
next paper) use NM to optimize at least one more thing: Make a histogram
of number of 10 second epochs on the vertical axis and Slow Wave Activity 
on the horizontal axis.  YOu should see two humps: one representing 
Wake and one for sleep.  Since the scoring of each epoch is something done 
by hand, (and prone to error), we'd like to get away from the human 
error element in that scoring and let the optimization algorithm 
choose the cut-off point between sleep and wake (or REM) and label all of 
epochs to the right of the cut-off as sleep and all epochs to the left of 
the cut-off as W(or R).  This would replace the human scoring of each 
epoch and it is something that Franken could not have done because computing 
power with brute force would have been prohibitive.  Also, later we could 
vary what exactly we mean by Slow Wave.  Is it 1-4 Hz or 0.5-5 Hz? or 1-9 Hz? 
We could perhaps vary this as well and see which one best fits the data. 
See the photo I took on June 13 2014.  Once a cut-off is chosen I would need to 
fill in the state column of the data file with the state score (S or W) made 
from looking at the algorithm and run the simulation (and optimize taui and taud) using 
that cut-off.

For the brute force calculations, use the same number of total iterations and 
ranges and increments as Franken did.  Franken says: "ti (1–25 hr, step-size 0.12 hr) 
and td (0.1–5 hr, step-size 0.025 hr) values, i.e., the simulation was run for 40,000 
different combinations of time constants for each mouse."  Make sure I use this range 
when doing the brute force calculations.  DONE in Franken_like_model.m.  I left the 
Figure2 script alone because the contour plot looks good as it is.  

Write a script to make the table for me: call ProcessLbatchmode.m on the BL strain 
and then compute mean and SEM for the tau values and mean and running time average and 
SEM.  I have to change PROCESSLBATCHMODE.m to use Franken_like_model_with_nelder_mead.m
instead of Franken_like_model.m I suppose I could put this in as an argument to PROCESSLBATCHMODE.m
then I could make one big script to make the entire table.  

TEST make_freq_plot.m by changing the number of bins used.  Does that affect the calculation of 
UA?  Perhaps this is the problem and why some SWA episodes of 5 minutes are above UA. 

Initial test of make_table.m didn't seem to work.  Fit of model to data for one BL file is terrible. 
Find out if normalizing UA and LA messed things up. WORK ON THIS.
Also look for a way to have all of my commits look like they came from me, not Jon. 


* Monday, June 16, 2014
Goals for today:
1) +Fix bug that messes up UA and LA in brute force.+ DONE
2) Get my WSU ID lined up. CALLED (IN PROGRESS)
3) Make sure my script for table is running properly. +CHANGE GRANULARITY BACK in Franken_like_model.m+ and +data_dir in make_table.m+
4) Figure out why some SWS episodes are above UA
5) Continue writing and working on to-do list in paper. 
6) +Make Nelder-Mead return XS and YS for both lactate and delta. Then plot+
   +them in Figure2_script.m.  I modified nelder_mead_for_delta.m to keep all the+ 
+iterates, so it should be possible to to plot them easily.+ DONE
 
  It looks like nelder_mead_for_delta.m stores the simplex in the variable V. make sure
it has three elements and figure out how to keep all 3 at every step so I can plot the 
triangles. Check size of V at end of algorithm (I added a size(V) statement to end of 
nelder_mead_for_delta.m)

For 1) NM still looks good with BL118140.txt.  Try Franken_like_model.m.  
CHANGE taui and taud BACK in Franken_like_model.m!! The starting value for S 
is different between NM and BF.  I'm trying to run BF with a finer spacing 
to see if that does it.  I can't see anything that would make them start at different S 
values.  They both call run_S_model.m which is where that is handled.  With a finer 
grid, Franken_like_model.m gives terrible results.  The values of taui and taud are fine, 
but the fit of the model to data is terrible.  S starts at 2000 instead of 3500.
I think I found the error: the problem was in the plot statement for plotting the best fit:
I was doing plot(t,S) not plot(t,best_S) at the end of Franken_like_model.m, so it was plotting the last iteration of S,
 which may have been way off.  Franken_like_model_with_nelder_mead.m already had 
plot(t,best_S) so that's why it was working and the other one was not.  FIXED

The problem file is BL-118540.txt.  Lactate is bad, hardly any change except sensor dying.  Check delta for this one. 

After meeting with Jonathan, we decided to set up a new protocol for handling lacate data sets. If the signal 
is lactate, find the first 2 NREM episodes that are 1 minute or longer and cut off all data before that.  
I have implemented this and it looks good. DONE
TO DO: I may want to update my plots so I'm plotting real time on the bottom 
axis, not time from start of simulation, since they all start at different times now.  
Check to see if the one file I use for the figure is affected. If so, I may have to slightly redo the panel in the 
figure. It is only minimally affected.  Not worth changing the figure. 

* Tuesday, June 17, 2014

some stuff

* Friday, July 11, 2014
Working on the automated scoring problem.  One approach I'm trying in PCA, like Gilmour_etal.  
I wrote sleepscorePCA.m and it runs and plots, but doesn't seem to give us nice separate clouds like
Gilmour shows.  If I used eig.m, subtracted out the row means, normalized and then added in row means again, I got
exactly what I got when I used eig.m without any normalizing. The graphs using pca.m and eigs with normalizing 
are not quite the same, altough qualitatively they are about the same.  Using eigs.m without any normalizing 
doesn't give me any data below zero. Why? Because all my data are positive (averages of EEG).
Gilmour has his variables going from -1 to 1 so I scaled my data to also go from -1 to 1. 
pca.m still gives me something slightly different than using eigs.m, but I think I will just stick with using pca.m
and normalizing to [-1 1] first. 
sleepscorePCA.m seems to be running well now and making nice graphs.  It doesn't match Gilmour though. Keep looking through
all the files in each of the strains.  

Try this routine on all the different strains to see if it looks good for any of them.  

If I 


Then keep working on the Bayesian approach (autoscore.m) using edfread.m and interpolating the lactate signal. 


* Monday, July 14, 2014
I have the PCA approach working, and I have an option to color the points based on human-scored sleep state.  The plots don't look 
much like the ones in Gilmour_et al, but they have some similarities.  
Strains:
AKR: not great
BA: a couple look OK
BL: not great
DBA: a couple were quite good, some bad 1671_05_09_2013 looks good

I was not able to compile the linux package that Will sent me (ParseNDP).  I may want to just look over the code to see if we can 
glean anything from it even if we can't actually run it.  Jon is working on it.  


On the front of using edfread.m and the Bayes autoscore.m function: I updated edfread.m to interpolate if a signal  
is sampled at a lower frequency than the others.  This seems to be working and it runs on BA1216_05_07_2012.edf and BA1209_3_16_2012.edf. 

I finally understand the header in the edf file: "samples" means the number of samples per epoch.  The length of the epoch (in seconds) in stored 
in "duration".  "records" is the number of "duration"-long epochs that the dataset contains.  So, the variable "record" that edfread.m returns has 
size "ns" rows and "records"*max(samples) columns.  

Check to make sure edfread.m works on other files and check efficiency vs. interp1qr.m then try running autoscore.m.  
RECHECK edfread.m again.  Now that I look at record it doesn't look right.  Lots of NaNs at the end.

* Tuesday, July 15, 2014
I'm getting NaNs at the end of the lactate signal.  For file BA1209_3_16_2012.edf the NaNs begin at location 28229002.  

It looks like I fixed this bug.  It was an indexing problem.  edfread.m seems to work properly now.  

* Wednesday, July 16, 2014
I tried testing my idea to set up a vector of timestamps to go along with the "records" data structure that edfread.m produces, 
but it was taking forever to run.  I had to kill it.  
New approach:  just compute the difference between the start time of the .txt file and the "records" data structure and 
cut off the appropriate number of columns of "records" so they match up.  This should be fast.  

Next make sure the lactate signal at the beginning of the cut-off records matches that of the .txt file (I'll need to 
average over 10 secods)
My testing framework including autoscore.m now runs, but the output is not good.  Check to make sure I have done the indexing correctly and the 
epochs are lining up.  If they were not, then the training data would be faulty, making autoscore.m bound to give bad results.  
Will says that the time listed is for the beginning of each epoch, they aren't centered.  Make sure the averages for the lactate signal
are working out correctly.  